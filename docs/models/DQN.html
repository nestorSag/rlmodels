<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>rlmodels.models.DQN API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rlmodels.models.DQN</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import copy
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from collections import deque

from .grad_utils import *

import logging

class DQNScheduler(object):
  &#34;&#34;&#34;double Q network hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global timestep counter.
  At each step it sets the hyperparameter values given by the provided functons

  **Parameters**:
  
  *batch_size_f* (*function*): batch size scheduler function

  *exploration_rate* (*function*): exploration rate scheduler function 

  *PER_alpha* (*function*): prioritised experience replay alpha scheduler function

  *PER_beta* (*function*): prioritised experience replay beta scheduler function

  *tau* (*function*): hard target update time window scheduler function

  *agent_lr_scheduler_fn* (*function*): multiplicative lr update for actor

  *steps_per_update* (*function*): number of SGD steps per update

  &#34;&#34;&#34;
  def __init__(
    self,
    batch_size,
    exploration_rate,
    PER_alpha,
    PER_beta,
    tau,
    agent_lr_scheduler_fn=None,
    steps_per_update=None):

    self.batch_size_f = batch_size
    self.exploration_rate_f = exploration_rate
    self.PER_alpha_f = PER_alpha
    self.tau_f = tau
    self.PER_beta_f = PER_beta
    self.steps_per_update_f = steps_per_update if steps_per_update is not None else lambda t: 1

    self.agent_lr_scheduler_fn = agent_lr_scheduler_fn

    self.reset()

  def _step(self):

    self.batch_size = self.batch_size_f(self.counter)
    self.exploration_rate = self.exploration_rate_f(self.counter)
    self.PER_alpha = self.PER_alpha_f(self.counter)
    self.tau = self.tau_f(self.counter)
    self.PER_beta = self.PER_beta_f(self.counter)
    self.steps_per_update = self.steps_per_update_f(self.counter)

    self.counter += 1

  def reset(self):

    &#34;&#34;&#34;reset iteration counter
  
    &#34;&#34;&#34;
    self.counter = 0

    self._step()

class DQN(object):
  &#34;&#34;&#34;double Q network with importance-sampled prioritised experienced replay (PER)

  **Parameters**:

  *agent* (`rlmodels.models.grad_utils.Agent`): model wrapper

  *env*: environment object with the same interface as OpenAI gym&#39;s environments

  *scheduler* (`DQNScheduler`): scheduler object that controls hyperparameter values at runtime

  &#34;&#34;&#34;
  def __init__(self,agent,env,scheduler):

    self.agent = agent
    self.env = env
    self.scheduler = scheduler
    self.mean_trace = []

    if self.scheduler.agent_lr_scheduler_fn is not None:
      self.agent.scheduler = optim.lr_scheduler.LambdaLR(self.agent.optim,self.scheduler.agent_lr_scheduler_fn)

  def _get_delta(
    self,
    agent1,
    agent2,
    batch,
    discount_rate,
    sample_weights,
    td_steps,
    optimise=True):

    #process batch
    S1 = torch.from_numpy(np.array([x[0] for x in batch])).float()
    A1 = torch.from_numpy(np.array([x[1] for x in batch])).long()
    R = torch.from_numpy(np.array([x[2] for x in batch])).float()
    S2 = torch.from_numpy(np.array([x[3] for x in batch])).float()
    T = torch.from_numpy(np.array([x[4] for x in batch])).float()

    with torch.no_grad():
      _, A2 = torch.max(agent1.forward(S2),1) #decide with q network

      Y = R.view(-1,1) + discount_rate**(td_steps)*agent2.forward(S2).gather(1,A2.view(-1,1))*T.view(-1,1) #evaluate with target network

    delta = Y - agent1.forward(S1).gather(1,A1.view(-1,1)) #optimise q network
    
    agent1.optim.zero_grad()
    if optimise:
      #optimise
      (sample_weights.view(-1,1)*delta**2).mean().backward()
      agent1.optim.step()

      if logging.getLogger().getEffectiveLevel() == logging.DEBUG: #additional logger calculations
        with torch.no_grad():
          delta2 = Y - agent1.forward(S1).gather(1,A1.view(-1,1)) #optimise q network
        logging.debug(&#34;mean loss improvement: {x}&#34;.format(x=torch.abs(delta).mean()-torch.abs(delta2).mean()))

    return torch.abs(delta).detach().numpy()

  def _step(self,agent,target,s1,eps,render=False):
    # perform an action given an agent, a target, the current state, and an epsilon (exploration probability)
    with torch.no_grad():
      q = agent.forward(s1)

    if np.random.binomial(1,eps,1)[0]:
      a = np.random.randint(low=0,high=list(q.shape)[0],size=1)[0]
    else:
      _, a = q.max(0) #argmax
      a = int(a.numpy())

    sarst = (s1,a) #t = termination

    s2,r,done,info = self.env.step(a)
    if render:
      self.env.render()
    sarst += (r,s2,1-int(done)) #t = termination signal (t=0 if s2 is terminal state, t=1 otherwise)

    return sarst

  def _process_td_steps(self,step_list,discount_rate):
    # takes a list of SARST steps as input and outputs an n-steps TD SARST tuple
    s0 = step_list[0][0]
    a = step_list[0][1]
    R = np.sum([step_list[i][2]*discount_rate**(i) for i in range(len(step_list))])
    s1 = step_list[-1][3]
    t = step_list[-1][4]

    return (s0,a,R,s1,t)

  def fit(
    self,
    n_episodes,
    max_ts_by_episode,
    discount_rate=0.99,
    max_memory_size=2000,
    td_steps = 1,
    reset=False,
    render = False):

    &#34;&#34;&#34;
    Fit the agent 
    
    **Parameters**:

    *n_episodes* (*int*): number of episodes to run

    *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode

    *discount_rate* (*float*): reward discount rate. Defaults to 0.99

    *max_memory_size* (*int*): max memory size for PER. Defaults to 2000

    *td_steps* (*int*): number of temporal difference steps to use in learning

    *reset* (*boolean*): reset trace, scheduler time counter and learning rate time counter to zero if fit has been called before

    *render* (*boolean*): render environment while fitting

    &#34;&#34;&#34;
    logging.info(&#34;Running DQN.fit&#34;)
    if reset:
      self.scheduler.reset()
      self.agent.scheduler = optim.scheduler.LambdaLR(self.agent.opt,self.scheduler.agent_lr_scheduler_fn)
      self.trace = []

    scheduler = self.scheduler

    agent = self.agent
    target = copy.deepcopy(agent)
    # initialize and fill memory 
    memory = SumTree(max_memory_size)

    s1 = self.env.reset()
    memsize = 0
    step_list = deque(maxlen=td_steps)
    td = 0 #temporal difference step counter

    logging.info(&#34;Filling memory...&#34;)
    while memsize &lt; max_memory_size:
      # fill step list
      step_list.append(self._step(agent,target,s1,scheduler.exploration_rate,False))
      s1 = step_list[-1][3]
      done = (step_list[-1][4] == 0)
      td += 1
      if td &gt;= td_steps:
        # compute temporal difference n-steps SARST
        td_sarst = self._process_td_steps(step_list,discount_rate)
        memory.add(1,td_sarst)
        memsize +=1

      if done:
        if td &lt; td_steps:
          td_sarst = self._process_td_steps(step_list,discount_rate)
          memory.add(1,td_sarst)
          memsize +=1
        # compute temporal difference n-steps SARST
        td = 0
        step_list = deque(maxlen=td_steps)
        s1 = self.env.reset()


    # fit agent
    logging.info(&#34;Training...&#34;)
    for i in tqdm(range(n_episodes)):
          
      s1 = self.env.reset()

      ts_reward = 0

      td = 0
      step_list = deque(maxlen=td_steps)

      for j in range(max_ts_by_episode):

        #execute policy
        step_list.append(self._step(agent,target,s1,scheduler.exploration_rate,render))
        td += 1
        s1 = step_list[-1][3]
        r = step_list[-1][2] #latest reward

        if np.isnan(r):
          raise RuntimeError(&#34;The model diverged; decreasing step sizes or increasing tau can help to prevent this.&#34;)

        done = (step_list[-1][4] == 0)

        if td &gt;= td_steps:
          # compute temporal difference n-steps SARST and its delta
          td_sarst = self._process_td_steps(step_list,discount_rate)
          delta = self._get_delta(agent,target,[td_sarst],discount_rate,torch.ones(1),td_steps,optimise=False)
          memory.add((delta[0] + 1.0/max_memory_size)**scheduler.PER_alpha,td_sarst)

        # sgd update
        # get replay batch
        P = memory.total()
        N = memory.get_current_size()
        if N &gt; 0:
          for h in range(scheduler.steps_per_update):

            try:
              samples = np.random.uniform(high=P,size=min(scheduler.batch_size,N))
            except OverflowError as e:
              print(e)
              print(&#34;it seems that the model parameters are diverging. Decreasing step size or increasing tau might help.&#34;)

            batch = []
            batch_ids = []
            batch_p = []
            for u in samples:
              idx, p ,data = memory.get(u)
              batch.append(data) #data from selected leaf
              batch_ids.append(idx)
              batch_p.append(p/P)

            #compute importance sampling weights
            batch_w = np.array(batch_p)
            batch_w = (1.0/(N*batch_w))**scheduler.PER_beta
            batch_w /= np.max(batch_w)
            batch_w = torch.from_numpy(batch_w).float()

            # perform optimisation
            delta = self._get_delta(agent,target,batch,discount_rate,batch_w,td_steps,optimise=True)

            #update memory
            for k in range(len(delta)):
              memory.update(batch_ids[k],(delta[k] + 1.0/max_memory_size)**scheduler.PER_alpha)

            #target network hard update
          if (scheduler.counter % scheduler.tau) == (scheduler.tau-1):
            target.model.load_state_dict(agent.model.state_dict())

        # trace information
        ts_reward += r

        # update learning rate and other hyperparameters
        agent._step()
        scheduler._step()

        if done:
          break

      if td &lt; td_steps:
        td_sarst = self._process_td_steps(step_list,discount_rate)
        delta = self._get_delta(agent,target,[td_sarst],discount_rate,torch.ones(1),td_steps,optimise=False)
        memory.add((delta[0] + 1.0/max_memory_size)**scheduler.PER_alpha,td_sarst)

      self.mean_trace.append(ts_reward)
      logging.info(&#34;episode {n}, timestep {ts}, mean reward {x}&#34;.format(n=i,x=ts_reward,ts=scheduler.counter))

    if render:
      self.env.close()

  def plot(self):
    &#34;&#34;&#34;plot mean timestep reward from last `fit` call

    &#34;&#34;&#34;

    if len(self.mean_trace)==0:
      print(&#34;The trace is empty.&#34;)
    else:
      df = pd.DataFrame({
        &#34;episode&#34;:list(range(len(self.mean_trace))),
        &#34;mean_reward&#34;: self.mean_trace})

    ax = sns.lineplot(data=df,x=&#34;episode&#34;,y=&#34;mean_reward&#34;)
    ax.set(xlabel=&#39;episode&#39;, ylabel=&#39;Mean episodic reward&#39;)
    plt.show()

  def play(self,n=200):
    &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
    
    **Parameters**:

    *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

    &#34;&#34;&#34;
    with torch.no_grad():
      obs = self.env.reset()
      for k in range(n):
        action = np.argmax(self.agent.model.forward(obs).numpy())
        obs,reward,done,info = self.env.step(action)
        self.env.render()
        if done:
          break
      self.env.close()

  def forward(self,x):
    &#34;&#34;&#34;evaluate input with agent

    **Parameters**:

    *x* (*torch.Tensor*): input vector

    &#34;&#34;&#34;
    if isinstance(x,np.ndarray):
      x = torch.from_numpy(x).float()
    return self.agent.model.forward(x)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rlmodels.models.DQN.DQN"><code class="flex name class">
<span>class <span class="ident">DQN</span></span>
<span>(</span><span>agent, env, scheduler)</span>
</code></dt>
<dd>
<div class="desc"><p>double Q network with importance-sampled prioritised experienced replay (PER)</p>
<p><strong>Parameters</strong>:</p>
<p><em>agent</em> (<code><a title="rlmodels.models.grad_utils.Agent" href="grad_utils.html#rlmodels.models.grad_utils.Agent">Agent</a></code>): model wrapper</p>
<p><em>env</em>: environment object with the same interface as OpenAI gym's environments</p>
<p><em>scheduler</em> (<code><a title="rlmodels.models.DQN.DQNScheduler" href="#rlmodels.models.DQN.DQNScheduler">DQNScheduler</a></code>): scheduler object that controls hyperparameter values at runtime</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DQN(object):
  &#34;&#34;&#34;double Q network with importance-sampled prioritised experienced replay (PER)

  **Parameters**:

  *agent* (`rlmodels.models.grad_utils.Agent`): model wrapper

  *env*: environment object with the same interface as OpenAI gym&#39;s environments

  *scheduler* (`DQNScheduler`): scheduler object that controls hyperparameter values at runtime

  &#34;&#34;&#34;
  def __init__(self,agent,env,scheduler):

    self.agent = agent
    self.env = env
    self.scheduler = scheduler
    self.mean_trace = []

    if self.scheduler.agent_lr_scheduler_fn is not None:
      self.agent.scheduler = optim.lr_scheduler.LambdaLR(self.agent.optim,self.scheduler.agent_lr_scheduler_fn)

  def _get_delta(
    self,
    agent1,
    agent2,
    batch,
    discount_rate,
    sample_weights,
    td_steps,
    optimise=True):

    #process batch
    S1 = torch.from_numpy(np.array([x[0] for x in batch])).float()
    A1 = torch.from_numpy(np.array([x[1] for x in batch])).long()
    R = torch.from_numpy(np.array([x[2] for x in batch])).float()
    S2 = torch.from_numpy(np.array([x[3] for x in batch])).float()
    T = torch.from_numpy(np.array([x[4] for x in batch])).float()

    with torch.no_grad():
      _, A2 = torch.max(agent1.forward(S2),1) #decide with q network

      Y = R.view(-1,1) + discount_rate**(td_steps)*agent2.forward(S2).gather(1,A2.view(-1,1))*T.view(-1,1) #evaluate with target network

    delta = Y - agent1.forward(S1).gather(1,A1.view(-1,1)) #optimise q network
    
    agent1.optim.zero_grad()
    if optimise:
      #optimise
      (sample_weights.view(-1,1)*delta**2).mean().backward()
      agent1.optim.step()

      if logging.getLogger().getEffectiveLevel() == logging.DEBUG: #additional logger calculations
        with torch.no_grad():
          delta2 = Y - agent1.forward(S1).gather(1,A1.view(-1,1)) #optimise q network
        logging.debug(&#34;mean loss improvement: {x}&#34;.format(x=torch.abs(delta).mean()-torch.abs(delta2).mean()))

    return torch.abs(delta).detach().numpy()

  def _step(self,agent,target,s1,eps,render=False):
    # perform an action given an agent, a target, the current state, and an epsilon (exploration probability)
    with torch.no_grad():
      q = agent.forward(s1)

    if np.random.binomial(1,eps,1)[0]:
      a = np.random.randint(low=0,high=list(q.shape)[0],size=1)[0]
    else:
      _, a = q.max(0) #argmax
      a = int(a.numpy())

    sarst = (s1,a) #t = termination

    s2,r,done,info = self.env.step(a)
    if render:
      self.env.render()
    sarst += (r,s2,1-int(done)) #t = termination signal (t=0 if s2 is terminal state, t=1 otherwise)

    return sarst

  def _process_td_steps(self,step_list,discount_rate):
    # takes a list of SARST steps as input and outputs an n-steps TD SARST tuple
    s0 = step_list[0][0]
    a = step_list[0][1]
    R = np.sum([step_list[i][2]*discount_rate**(i) for i in range(len(step_list))])
    s1 = step_list[-1][3]
    t = step_list[-1][4]

    return (s0,a,R,s1,t)

  def fit(
    self,
    n_episodes,
    max_ts_by_episode,
    discount_rate=0.99,
    max_memory_size=2000,
    td_steps = 1,
    reset=False,
    render = False):

    &#34;&#34;&#34;
    Fit the agent 
    
    **Parameters**:

    *n_episodes* (*int*): number of episodes to run

    *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode

    *discount_rate* (*float*): reward discount rate. Defaults to 0.99

    *max_memory_size* (*int*): max memory size for PER. Defaults to 2000

    *td_steps* (*int*): number of temporal difference steps to use in learning

    *reset* (*boolean*): reset trace, scheduler time counter and learning rate time counter to zero if fit has been called before

    *render* (*boolean*): render environment while fitting

    &#34;&#34;&#34;
    logging.info(&#34;Running DQN.fit&#34;)
    if reset:
      self.scheduler.reset()
      self.agent.scheduler = optim.scheduler.LambdaLR(self.agent.opt,self.scheduler.agent_lr_scheduler_fn)
      self.trace = []

    scheduler = self.scheduler

    agent = self.agent
    target = copy.deepcopy(agent)
    # initialize and fill memory 
    memory = SumTree(max_memory_size)

    s1 = self.env.reset()
    memsize = 0
    step_list = deque(maxlen=td_steps)
    td = 0 #temporal difference step counter

    logging.info(&#34;Filling memory...&#34;)
    while memsize &lt; max_memory_size:
      # fill step list
      step_list.append(self._step(agent,target,s1,scheduler.exploration_rate,False))
      s1 = step_list[-1][3]
      done = (step_list[-1][4] == 0)
      td += 1
      if td &gt;= td_steps:
        # compute temporal difference n-steps SARST
        td_sarst = self._process_td_steps(step_list,discount_rate)
        memory.add(1,td_sarst)
        memsize +=1

      if done:
        if td &lt; td_steps:
          td_sarst = self._process_td_steps(step_list,discount_rate)
          memory.add(1,td_sarst)
          memsize +=1
        # compute temporal difference n-steps SARST
        td = 0
        step_list = deque(maxlen=td_steps)
        s1 = self.env.reset()


    # fit agent
    logging.info(&#34;Training...&#34;)
    for i in tqdm(range(n_episodes)):
          
      s1 = self.env.reset()

      ts_reward = 0

      td = 0
      step_list = deque(maxlen=td_steps)

      for j in range(max_ts_by_episode):

        #execute policy
        step_list.append(self._step(agent,target,s1,scheduler.exploration_rate,render))
        td += 1
        s1 = step_list[-1][3]
        r = step_list[-1][2] #latest reward

        if np.isnan(r):
          raise RuntimeError(&#34;The model diverged; decreasing step sizes or increasing tau can help to prevent this.&#34;)

        done = (step_list[-1][4] == 0)

        if td &gt;= td_steps:
          # compute temporal difference n-steps SARST and its delta
          td_sarst = self._process_td_steps(step_list,discount_rate)
          delta = self._get_delta(agent,target,[td_sarst],discount_rate,torch.ones(1),td_steps,optimise=False)
          memory.add((delta[0] + 1.0/max_memory_size)**scheduler.PER_alpha,td_sarst)

        # sgd update
        # get replay batch
        P = memory.total()
        N = memory.get_current_size()
        if N &gt; 0:
          for h in range(scheduler.steps_per_update):

            try:
              samples = np.random.uniform(high=P,size=min(scheduler.batch_size,N))
            except OverflowError as e:
              print(e)
              print(&#34;it seems that the model parameters are diverging. Decreasing step size or increasing tau might help.&#34;)

            batch = []
            batch_ids = []
            batch_p = []
            for u in samples:
              idx, p ,data = memory.get(u)
              batch.append(data) #data from selected leaf
              batch_ids.append(idx)
              batch_p.append(p/P)

            #compute importance sampling weights
            batch_w = np.array(batch_p)
            batch_w = (1.0/(N*batch_w))**scheduler.PER_beta
            batch_w /= np.max(batch_w)
            batch_w = torch.from_numpy(batch_w).float()

            # perform optimisation
            delta = self._get_delta(agent,target,batch,discount_rate,batch_w,td_steps,optimise=True)

            #update memory
            for k in range(len(delta)):
              memory.update(batch_ids[k],(delta[k] + 1.0/max_memory_size)**scheduler.PER_alpha)

            #target network hard update
          if (scheduler.counter % scheduler.tau) == (scheduler.tau-1):
            target.model.load_state_dict(agent.model.state_dict())

        # trace information
        ts_reward += r

        # update learning rate and other hyperparameters
        agent._step()
        scheduler._step()

        if done:
          break

      if td &lt; td_steps:
        td_sarst = self._process_td_steps(step_list,discount_rate)
        delta = self._get_delta(agent,target,[td_sarst],discount_rate,torch.ones(1),td_steps,optimise=False)
        memory.add((delta[0] + 1.0/max_memory_size)**scheduler.PER_alpha,td_sarst)

      self.mean_trace.append(ts_reward)
      logging.info(&#34;episode {n}, timestep {ts}, mean reward {x}&#34;.format(n=i,x=ts_reward,ts=scheduler.counter))

    if render:
      self.env.close()

  def plot(self):
    &#34;&#34;&#34;plot mean timestep reward from last `fit` call

    &#34;&#34;&#34;

    if len(self.mean_trace)==0:
      print(&#34;The trace is empty.&#34;)
    else:
      df = pd.DataFrame({
        &#34;episode&#34;:list(range(len(self.mean_trace))),
        &#34;mean_reward&#34;: self.mean_trace})

    ax = sns.lineplot(data=df,x=&#34;episode&#34;,y=&#34;mean_reward&#34;)
    ax.set(xlabel=&#39;episode&#39;, ylabel=&#39;Mean episodic reward&#39;)
    plt.show()

  def play(self,n=200):
    &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
    
    **Parameters**:

    *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

    &#34;&#34;&#34;
    with torch.no_grad():
      obs = self.env.reset()
      for k in range(n):
        action = np.argmax(self.agent.model.forward(obs).numpy())
        obs,reward,done,info = self.env.step(action)
        self.env.render()
        if done:
          break
      self.env.close()

  def forward(self,x):
    &#34;&#34;&#34;evaluate input with agent

    **Parameters**:

    *x* (*torch.Tensor*): input vector

    &#34;&#34;&#34;
    if isinstance(x,np.ndarray):
      x = torch.from_numpy(x).float()
    return self.agent.model.forward(x)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="rlmodels.models.DQN.DQN.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, n_episodes, max_ts_by_episode, discount_rate=0.99, max_memory_size=2000, td_steps=1, reset=False, render=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit the agent </p>
<p><strong>Parameters</strong>:</p>
<p><em>n_episodes</em> (<em>int</em>): number of episodes to run</p>
<p><em>max_ts_by_episodes</em> (<em>int</em>): maximum number of timesteps to run per episode</p>
<p><em>discount_rate</em> (<em>float</em>): reward discount rate. Defaults to 0.99</p>
<p><em>max_memory_size</em> (<em>int</em>): max memory size for PER. Defaults to 2000</p>
<p><em>td_steps</em> (<em>int</em>): number of temporal difference steps to use in learning</p>
<p><em>reset</em> (<em>boolean</em>): reset trace, scheduler time counter and learning rate time counter to zero if fit has been called before</p>
<p><em>render</em> (<em>boolean</em>): render environment while fitting</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(
  self,
  n_episodes,
  max_ts_by_episode,
  discount_rate=0.99,
  max_memory_size=2000,
  td_steps = 1,
  reset=False,
  render = False):

  &#34;&#34;&#34;
  Fit the agent 
  
  **Parameters**:

  *n_episodes* (*int*): number of episodes to run

  *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode

  *discount_rate* (*float*): reward discount rate. Defaults to 0.99

  *max_memory_size* (*int*): max memory size for PER. Defaults to 2000

  *td_steps* (*int*): number of temporal difference steps to use in learning

  *reset* (*boolean*): reset trace, scheduler time counter and learning rate time counter to zero if fit has been called before

  *render* (*boolean*): render environment while fitting

  &#34;&#34;&#34;
  logging.info(&#34;Running DQN.fit&#34;)
  if reset:
    self.scheduler.reset()
    self.agent.scheduler = optim.scheduler.LambdaLR(self.agent.opt,self.scheduler.agent_lr_scheduler_fn)
    self.trace = []

  scheduler = self.scheduler

  agent = self.agent
  target = copy.deepcopy(agent)
  # initialize and fill memory 
  memory = SumTree(max_memory_size)

  s1 = self.env.reset()
  memsize = 0
  step_list = deque(maxlen=td_steps)
  td = 0 #temporal difference step counter

  logging.info(&#34;Filling memory...&#34;)
  while memsize &lt; max_memory_size:
    # fill step list
    step_list.append(self._step(agent,target,s1,scheduler.exploration_rate,False))
    s1 = step_list[-1][3]
    done = (step_list[-1][4] == 0)
    td += 1
    if td &gt;= td_steps:
      # compute temporal difference n-steps SARST
      td_sarst = self._process_td_steps(step_list,discount_rate)
      memory.add(1,td_sarst)
      memsize +=1

    if done:
      if td &lt; td_steps:
        td_sarst = self._process_td_steps(step_list,discount_rate)
        memory.add(1,td_sarst)
        memsize +=1
      # compute temporal difference n-steps SARST
      td = 0
      step_list = deque(maxlen=td_steps)
      s1 = self.env.reset()


  # fit agent
  logging.info(&#34;Training...&#34;)
  for i in tqdm(range(n_episodes)):
        
    s1 = self.env.reset()

    ts_reward = 0

    td = 0
    step_list = deque(maxlen=td_steps)

    for j in range(max_ts_by_episode):

      #execute policy
      step_list.append(self._step(agent,target,s1,scheduler.exploration_rate,render))
      td += 1
      s1 = step_list[-1][3]
      r = step_list[-1][2] #latest reward

      if np.isnan(r):
        raise RuntimeError(&#34;The model diverged; decreasing step sizes or increasing tau can help to prevent this.&#34;)

      done = (step_list[-1][4] == 0)

      if td &gt;= td_steps:
        # compute temporal difference n-steps SARST and its delta
        td_sarst = self._process_td_steps(step_list,discount_rate)
        delta = self._get_delta(agent,target,[td_sarst],discount_rate,torch.ones(1),td_steps,optimise=False)
        memory.add((delta[0] + 1.0/max_memory_size)**scheduler.PER_alpha,td_sarst)

      # sgd update
      # get replay batch
      P = memory.total()
      N = memory.get_current_size()
      if N &gt; 0:
        for h in range(scheduler.steps_per_update):

          try:
            samples = np.random.uniform(high=P,size=min(scheduler.batch_size,N))
          except OverflowError as e:
            print(e)
            print(&#34;it seems that the model parameters are diverging. Decreasing step size or increasing tau might help.&#34;)

          batch = []
          batch_ids = []
          batch_p = []
          for u in samples:
            idx, p ,data = memory.get(u)
            batch.append(data) #data from selected leaf
            batch_ids.append(idx)
            batch_p.append(p/P)

          #compute importance sampling weights
          batch_w = np.array(batch_p)
          batch_w = (1.0/(N*batch_w))**scheduler.PER_beta
          batch_w /= np.max(batch_w)
          batch_w = torch.from_numpy(batch_w).float()

          # perform optimisation
          delta = self._get_delta(agent,target,batch,discount_rate,batch_w,td_steps,optimise=True)

          #update memory
          for k in range(len(delta)):
            memory.update(batch_ids[k],(delta[k] + 1.0/max_memory_size)**scheduler.PER_alpha)

          #target network hard update
        if (scheduler.counter % scheduler.tau) == (scheduler.tau-1):
          target.model.load_state_dict(agent.model.state_dict())

      # trace information
      ts_reward += r

      # update learning rate and other hyperparameters
      agent._step()
      scheduler._step()

      if done:
        break

    if td &lt; td_steps:
      td_sarst = self._process_td_steps(step_list,discount_rate)
      delta = self._get_delta(agent,target,[td_sarst],discount_rate,torch.ones(1),td_steps,optimise=False)
      memory.add((delta[0] + 1.0/max_memory_size)**scheduler.PER_alpha,td_sarst)

    self.mean_trace.append(ts_reward)
    logging.info(&#34;episode {n}, timestep {ts}, mean reward {x}&#34;.format(n=i,x=ts_reward,ts=scheduler.counter))

  if render:
    self.env.close()</code></pre>
</details>
</dd>
<dt id="rlmodels.models.DQN.DQN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>evaluate input with agent</p>
<p><strong>Parameters</strong>:</p>
<p><em>x</em> (<em>torch.Tensor</em>): input vector</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,x):
  &#34;&#34;&#34;evaluate input with agent

  **Parameters**:

  *x* (*torch.Tensor*): input vector

  &#34;&#34;&#34;
  if isinstance(x,np.ndarray):
    x = torch.from_numpy(x).float()
  return self.agent.model.forward(x)</code></pre>
</details>
</dd>
<dt id="rlmodels.models.DQN.DQN.play"><code class="name flex">
<span>def <span class="ident">play</span></span>(<span>self, n=200)</span>
</code></dt>
<dd>
<div class="desc"><p>show agent's animation. Only works for OpenAI environments</p>
<p><strong>Parameters</strong>:</p>
<p><em>n</em> (<em>int</em>): maximum number of timesteps to visualise. Defaults to 200</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def play(self,n=200):
  &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
  
  **Parameters**:

  *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

  &#34;&#34;&#34;
  with torch.no_grad():
    obs = self.env.reset()
    for k in range(n):
      action = np.argmax(self.agent.model.forward(obs).numpy())
      obs,reward,done,info = self.env.step(action)
      self.env.render()
      if done:
        break
    self.env.close()</code></pre>
</details>
</dd>
<dt id="rlmodels.models.DQN.DQN.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>plot mean timestep reward from last <code>fit</code> call</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self):
  &#34;&#34;&#34;plot mean timestep reward from last `fit` call

  &#34;&#34;&#34;

  if len(self.mean_trace)==0:
    print(&#34;The trace is empty.&#34;)
  else:
    df = pd.DataFrame({
      &#34;episode&#34;:list(range(len(self.mean_trace))),
      &#34;mean_reward&#34;: self.mean_trace})

  ax = sns.lineplot(data=df,x=&#34;episode&#34;,y=&#34;mean_reward&#34;)
  ax.set(xlabel=&#39;episode&#39;, ylabel=&#39;Mean episodic reward&#39;)
  plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="rlmodels.models.DQN.DQNScheduler"><code class="flex name class">
<span>class <span class="ident">DQNScheduler</span></span>
<span>(</span><span>batch_size, exploration_rate, PER_alpha, PER_beta, tau, agent_lr_scheduler_fn=None, steps_per_update=None)</span>
</code></dt>
<dd>
<div class="desc"><p>double Q network hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global timestep counter.
At each step it sets the hyperparameter values given by the provided functons</p>
<p><strong>Parameters</strong>:</p>
<p><em>batch_size_f</em> (<em>function</em>): batch size scheduler function</p>
<p><em>exploration_rate</em> (<em>function</em>): exploration rate scheduler function </p>
<p><em>PER_alpha</em> (<em>function</em>): prioritised experience replay alpha scheduler function</p>
<p><em>PER_beta</em> (<em>function</em>): prioritised experience replay beta scheduler function</p>
<p><em>tau</em> (<em>function</em>): hard target update time window scheduler function</p>
<p><em>agent_lr_scheduler_fn</em> (<em>function</em>): multiplicative lr update for actor</p>
<p><em>steps_per_update</em> (<em>function</em>): number of SGD steps per update</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DQNScheduler(object):
  &#34;&#34;&#34;double Q network hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global timestep counter.
  At each step it sets the hyperparameter values given by the provided functons

  **Parameters**:
  
  *batch_size_f* (*function*): batch size scheduler function

  *exploration_rate* (*function*): exploration rate scheduler function 

  *PER_alpha* (*function*): prioritised experience replay alpha scheduler function

  *PER_beta* (*function*): prioritised experience replay beta scheduler function

  *tau* (*function*): hard target update time window scheduler function

  *agent_lr_scheduler_fn* (*function*): multiplicative lr update for actor

  *steps_per_update* (*function*): number of SGD steps per update

  &#34;&#34;&#34;
  def __init__(
    self,
    batch_size,
    exploration_rate,
    PER_alpha,
    PER_beta,
    tau,
    agent_lr_scheduler_fn=None,
    steps_per_update=None):

    self.batch_size_f = batch_size
    self.exploration_rate_f = exploration_rate
    self.PER_alpha_f = PER_alpha
    self.tau_f = tau
    self.PER_beta_f = PER_beta
    self.steps_per_update_f = steps_per_update if steps_per_update is not None else lambda t: 1

    self.agent_lr_scheduler_fn = agent_lr_scheduler_fn

    self.reset()

  def _step(self):

    self.batch_size = self.batch_size_f(self.counter)
    self.exploration_rate = self.exploration_rate_f(self.counter)
    self.PER_alpha = self.PER_alpha_f(self.counter)
    self.tau = self.tau_f(self.counter)
    self.PER_beta = self.PER_beta_f(self.counter)
    self.steps_per_update = self.steps_per_update_f(self.counter)

    self.counter += 1

  def reset(self):

    &#34;&#34;&#34;reset iteration counter
  
    &#34;&#34;&#34;
    self.counter = 0

    self._step()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="rlmodels.models.DQN.DQNScheduler.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>reset iteration counter</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):

  &#34;&#34;&#34;reset iteration counter

  &#34;&#34;&#34;
  self.counter = 0

  self._step()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rlmodels.models" href="index.html">rlmodels.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rlmodels.models.DQN.DQN" href="#rlmodels.models.DQN.DQN">DQN</a></code></h4>
<ul class="">
<li><code><a title="rlmodels.models.DQN.DQN.fit" href="#rlmodels.models.DQN.DQN.fit">fit</a></code></li>
<li><code><a title="rlmodels.models.DQN.DQN.forward" href="#rlmodels.models.DQN.DQN.forward">forward</a></code></li>
<li><code><a title="rlmodels.models.DQN.DQN.play" href="#rlmodels.models.DQN.DQN.play">play</a></code></li>
<li><code><a title="rlmodels.models.DQN.DQN.plot" href="#rlmodels.models.DQN.DQN.plot">plot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="rlmodels.models.DQN.DQNScheduler" href="#rlmodels.models.DQN.DQNScheduler">DQNScheduler</a></code></h4>
<ul class="">
<li><code><a title="rlmodels.models.DQN.DQNScheduler.reset" href="#rlmodels.models.DQN.DQNScheduler.reset">reset</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>