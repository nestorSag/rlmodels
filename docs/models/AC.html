<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>rlmodels.models.AC API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rlmodels.models.AC</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python">import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from .grad_utils import *

import logging

from collections import deque

class ACScheduler(object):
  &#34;&#34;&#34;AC hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global timestep counter.
  At each step it sets the hyperparameter values given by the provided functons

  **Parameters**:
  
  *actor_lr_scheduler_func* (*function*): multiplicative lr update for actor

  *critic_lr_scheduler_func* (*function*): multiplicative lr update for critic

  &#34;&#34;&#34;
  def __init__(
    self,
    actor_lr_scheduler_fn = None,
    critic_lr_scheduler_fn = None):

    self.critic_lr_scheduler_fn = critic_lr_scheduler_fn
    self.actor_lr_scheduler_fn = actor_lr_scheduler_fn

    self.reset()

  def _step(self):

    self.counter += 1

  def reset(self):

    &#34;&#34;&#34;reset iteration counter
  
    &#34;&#34;&#34;
    self.counter = 0

    self._step()


class AC(object):
  &#34;&#34;&#34;actor-critic policy gradient algorithm

  **Parameters**:

  *actor* ( `rlmodels.models.grad_utils.Agent` ): model wrapper. Its *model* attribute must be a network that returns a suitable *torch.distributions* object from which to sample actions

  *critic* (`rlmodels.models.grad_utils.Agent`): model wrapper

  *env*: environment object with the same interface as OpenAI gym&#39;s environments

  *scheduler* (`ACScheduler`): scheduler object that controls hyperparameter values at runtime

  &#34;&#34;&#34;
  def __init__(self,actor,critic,env,scheduler):

    self.actor = actor
    self.critic = critic
    self.critic.loss = nn.MSELoss()

    self.env = env

    if len(self.env.action_space.shape) == 0:
      self.action_is_discrete = True
    else:
      self.action_is_discrete = False
      #get action space boundaries
      self.action_high = torch.from_numpy(env.action_space.high).float()
      self.action_low = torch.from_numpy(env.action_space.low).float()


    self.scheduler = scheduler
    self.mean_trace = []

    if self.scheduler.critic_lr_scheduler_fn is not None:
      self.critic.scheduler = optim.lr_scheduler.LambdaLR(self.critic.optim,self.scheduler.critic_lr_scheduler_fn)

    if self.scheduler.actor_lr_scheduler_fn is not None:
      self.actor.scheduler = optim.lr_scheduler.LambdaLR(self.actor.optim,self.scheduler.actor_lr_scheduler_fn)

  def _update(
    self,
    actor,
    critic,
    batch,
    discount_rate,
    optimise=True):

    # return delta = PER weights. if optimise = True, agents are updated in-place
    batch_size = len(batch)

    #process batch
    S1 = torch.from_numpy(np.array([x[0] for x in batch])).float()
    A1 = torch.from_numpy(np.array([x[1] for x in batch])).view(-1,1)
    R = torch.from_numpy(np.array([x[2] for x in batch])).float()
    S2 = torch.from_numpy(np.array([x[3] for x in batch])).float()
    T = torch.from_numpy(np.array([x[4] for x in batch])).float()

    time_idx = np.array(range(batch_size))
    step_discounts = torch.from_numpy(discount_rate**((1+time_idx)[::-1])).float().view(-1,1)

    critic.optim.zero_grad()
    with torch.no_grad():
      Y = R.view(-1,1) + step_discounts*critic.forward(S2)*T.view(-1,1) #evaluate with target network

    delta = Y - critic.forward(S1)

    #optimise critic
    if optimise:
      (delta**2).mean().backward() #weighted loss
      #torch.nn.utils.clip_grad_value_(critic.model.parameters(), 1) #clip gradient
      critic.optim.step()

      if logging.getLogger().getEffectiveLevel() == logging.DEBUG: #additional debug computations
        with torch.no_grad():
          delta2 = Y - critic.forward(S1)
          improvement = np.mean(torch.abs(delta).detach().numpy()) - np.mean(torch.abs(delta2).detach().numpy())
        logging.debug(&#34;Critic mean loss improvement: {x}&#34;.format(x=improvement))

    actor.optim.zero_grad()
    delta = delta.detach()
    # optimise actor
    if optimise:
      pi = actor.forward(S1)
      pg = - (pi.log_prob(A1)*delta).mean()
      pg.backward()
      #torch.nn.utils.clip_grad_value_(actor.model.parameters(), 1) #clip gradient; normal policies tend to have exploding gradients
      actor.optim.step()

      if logging.getLogger().getEffectiveLevel() == logging.DEBUG: #additional debug computations
        with torch.no_grad():
          pi = actor.forward(S1)
          pg2 = - (pi.log_prob(A1)*(delta.detach())).mean()
          logging.debug(&#34;pi: {x}&#34;.format(x=pi))
          logging.debug(&#34;action: {x}&#34;.format(x=A1))
          logging.debug(&#34;weighted log density change: {x}&#34;.format(x=pg-pg2))

  def _step(self,actor, critic,s1,render):
    # perform an action given an actor and critic, the current state, and an epsilon (exploration probability)
    with torch.no_grad():
      dist = actor.forward(s1)
      #print(dist.probs)
      a = dist.sample()
      if self.action_is_discrete:
        a = int(a)
      else:
        a = torch.min(torch.max(a,self.action_low),self.action_high) #clip action

    sarst = (s1,a) #t = termination

    s2,r,done,info = self.env.step(a)
    if render:
      self.env.render()
    sarst += (float(r),s2,1-int(done)) #t = termination signal (t=0 if s2 is terminal state, t=1 otherwise)

    return sarst

  def _process_td_steps(self,step_list,discount_rate):
    # takes a list of SARST steps as input and outputs an n-steps TD SARST tuple
    s0 = step_list[0][0]
    a = step_list[0][1]
    R = np.sum([step_list[i][2]*discount_rate**(i) for i in range(len(step_list))])
    s1 = step_list[-1][3]
    t = step_list[-1][4]

    return (s0,a,R,s1,t)

  def fit(
    self,
    n_episodes,
    max_ts_by_episode,
    discount_rate=0.99,
    tmax = 1,
    reset=False,
    render=False):

    &#34;&#34;&#34;
    Fit the agent 
    
    **Parameters**:

    *n_episodes* (int): number of episodes to run

    *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode

    *discount_rate* (*float*): reward discount rate. Defaults to 0.99

    *tmax* (*int*): number of timesteps between k-step TD updates, k=1,...,tmax. An infinite value implies episodic updates

    *reset* (*boolean*): reset trace, scheduler time counter and learning rate time counter to zero if fit has been called before

    *render* (*boolean*): render environment while fitting

    **Returns**:

    (nn.Module) updated agent

    &#34;&#34;&#34;
    logging.info(&#34;Running AC.fit&#34;)
    if reset:
      self.scheduler.reset()
      self.trace = []
      self.critic.scheduler = optim.scheduler.LambdaLR(self.critic.opt,self.scheduler.critic_lr_scheduler_fn)
      self.actor.scheduler = optim.scheduler.LambdaLR(self.actor.opt,self.scheduler.actor_lr_scheduler_fn)

    scheduler = self.scheduler
    actor = self.actor
    critic = self.critic

    # fit agents
    logging.info(&#34;Training...&#34;)

    for i in range(n_episodes):

      ts_reward = 0

      step_list = deque(maxlen=tmax if isinstance(tmax,int) else None)
      td = 0

      s1 = self.env.reset()

      for j in range(max_ts_by_episode):

        #execute policy
        step_list.append(self._step(actor,critic,s1,render))
        td += 1

        s1 = step_list[-1][3]
        r = step_list[-1][2] #latest reward

        if np.isnan(r):
          raise Exception(&#34;The model diverged; decreasing step sizes or increasing the minimum allowed variance of continuous policies can help to prevent this.&#34;)
        
        done = (step_list[-1][4] == 0)

        if td == tmax:
          processed = [self._process_td_steps([step_list[j] for j in range(i,tmax)],discount_rate) for i in range(tmax)]
          self._update(actor,critic,processed,discount_rate,optimise=True)
          td = 0

        # trace information
        ts_reward += r

        # update learning rate and other hyperparameters
        actor._step()
        critic._step()
        scheduler._step()

        if done:

          break

      if td &gt; 0:
        processed = [self._process_td_steps([step_list[-td+j] for j in range(i,td)],discount_rate) for i in range(td)]
        self._update(actor,critic,processed,discount_rate,optimise=True)

      self.mean_trace.append(ts_reward)
      logging.info(&#34;episode {n}, timestep {ts}, mean reward {x}&#34;.format(n=i,x=ts_reward,ts=scheduler.counter))

    self.actor = actor
    self.critic = critic

    if render:
      self.env.close()

  def plot(self):
    &#34;&#34;&#34;plot mean timestep reward from last `fit` call

    &#34;&#34;&#34;

    if len(self.mean_trace)==0:
      print(&#34;The trace is empty.&#34;)
    else:
      df = pd.DataFrame({
        &#34;episode&#34;:list(range(len(self.mean_trace))),
        &#34;mean_reward&#34;: self.mean_trace})

    ax = sns.lineplot(data=df,x=&#34;episode&#34;,y=&#34;mean_reward&#34;)
    ax.set(xlabel=&#39;episode&#39;, ylabel=&#39;Mean episodic reward&#39;)
    plt.show()

  def play(self,n=200):
    &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
    
    **Parameters**:

    *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

    &#34;&#34;&#34;
    with torch.no_grad():
      obs = self.env.reset()
      for k in range(n):
        pi = self.actor.model.forward(obs)
        action = pi.sample()
        if self.action_is_discrete:
          action = int(action)
        obs,reward,done,info = self.env.step(action)
        self.env.render()
        if done:
          break
      self.env.close()

  def forward(self,x):
    &#34;&#34;&#34;evaluate input with agent

    **Parameters**:

    *x* (*torch.Tensor*): input vector

    &#34;&#34;&#34;
    if isinstance(x,np.ndarray):
      x = torch.from_numpy(x).float()
    return self.agent.model.forward(x)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rlmodels.models.AC.AC"><code class="flex name class">
<span>class <span class="ident">AC</span></span>
<span>(</span><span>actor, critic, env, scheduler)</span>
</code></dt>
<dd>
<section class="desc"><p>actor-critic policy gradient algorithm</p>
<p><strong>Parameters</strong>:</p>
<p><em>actor</em> ( <a title="rlmodels.models.grad_utils.Agent" href="grad_utils.html#rlmodels.models.grad_utils.Agent"><code>Agent</code></a> ): model wrapper. Its <em>model</em> attribute must be a network that returns a suitable <em>torch.distributions</em> object from which to sample actions</p>
<p><em>critic</em> (<a title="rlmodels.models.grad_utils.Agent" href="grad_utils.html#rlmodels.models.grad_utils.Agent"><code>Agent</code></a>): model wrapper</p>
<p><em>env</em>: environment object with the same interface as OpenAI gym's environments</p>
<p><em>scheduler</em> (<a title="rlmodels.models.AC.ACScheduler" href="#rlmodels.models.AC.ACScheduler"><code>ACScheduler</code></a>): scheduler object that controls hyperparameter values at runtime</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class AC(object):
  &#34;&#34;&#34;actor-critic policy gradient algorithm

  **Parameters**:

  *actor* ( `rlmodels.models.grad_utils.Agent` ): model wrapper. Its *model* attribute must be a network that returns a suitable *torch.distributions* object from which to sample actions

  *critic* (`rlmodels.models.grad_utils.Agent`): model wrapper

  *env*: environment object with the same interface as OpenAI gym&#39;s environments

  *scheduler* (`ACScheduler`): scheduler object that controls hyperparameter values at runtime

  &#34;&#34;&#34;
  def __init__(self,actor,critic,env,scheduler):

    self.actor = actor
    self.critic = critic
    self.critic.loss = nn.MSELoss()

    self.env = env

    if len(self.env.action_space.shape) == 0:
      self.action_is_discrete = True
    else:
      self.action_is_discrete = False
      #get action space boundaries
      self.action_high = torch.from_numpy(env.action_space.high).float()
      self.action_low = torch.from_numpy(env.action_space.low).float()


    self.scheduler = scheduler
    self.mean_trace = []

    if self.scheduler.critic_lr_scheduler_fn is not None:
      self.critic.scheduler = optim.lr_scheduler.LambdaLR(self.critic.optim,self.scheduler.critic_lr_scheduler_fn)

    if self.scheduler.actor_lr_scheduler_fn is not None:
      self.actor.scheduler = optim.lr_scheduler.LambdaLR(self.actor.optim,self.scheduler.actor_lr_scheduler_fn)

  def _update(
    self,
    actor,
    critic,
    batch,
    discount_rate,
    optimise=True):

    # return delta = PER weights. if optimise = True, agents are updated in-place
    batch_size = len(batch)

    #process batch
    S1 = torch.from_numpy(np.array([x[0] for x in batch])).float()
    A1 = torch.from_numpy(np.array([x[1] for x in batch])).view(-1,1)
    R = torch.from_numpy(np.array([x[2] for x in batch])).float()
    S2 = torch.from_numpy(np.array([x[3] for x in batch])).float()
    T = torch.from_numpy(np.array([x[4] for x in batch])).float()

    time_idx = np.array(range(batch_size))
    step_discounts = torch.from_numpy(discount_rate**((1+time_idx)[::-1])).float().view(-1,1)

    critic.optim.zero_grad()
    with torch.no_grad():
      Y = R.view(-1,1) + step_discounts*critic.forward(S2)*T.view(-1,1) #evaluate with target network

    delta = Y - critic.forward(S1)

    #optimise critic
    if optimise:
      (delta**2).mean().backward() #weighted loss
      #torch.nn.utils.clip_grad_value_(critic.model.parameters(), 1) #clip gradient
      critic.optim.step()

      if logging.getLogger().getEffectiveLevel() == logging.DEBUG: #additional debug computations
        with torch.no_grad():
          delta2 = Y - critic.forward(S1)
          improvement = np.mean(torch.abs(delta).detach().numpy()) - np.mean(torch.abs(delta2).detach().numpy())
        logging.debug(&#34;Critic mean loss improvement: {x}&#34;.format(x=improvement))

    actor.optim.zero_grad()
    delta = delta.detach()
    # optimise actor
    if optimise:
      pi = actor.forward(S1)
      pg = - (pi.log_prob(A1)*delta).mean()
      pg.backward()
      #torch.nn.utils.clip_grad_value_(actor.model.parameters(), 1) #clip gradient; normal policies tend to have exploding gradients
      actor.optim.step()

      if logging.getLogger().getEffectiveLevel() == logging.DEBUG: #additional debug computations
        with torch.no_grad():
          pi = actor.forward(S1)
          pg2 = - (pi.log_prob(A1)*(delta.detach())).mean()
          logging.debug(&#34;pi: {x}&#34;.format(x=pi))
          logging.debug(&#34;action: {x}&#34;.format(x=A1))
          logging.debug(&#34;weighted log density change: {x}&#34;.format(x=pg-pg2))

  def _step(self,actor, critic,s1,render):
    # perform an action given an actor and critic, the current state, and an epsilon (exploration probability)
    with torch.no_grad():
      dist = actor.forward(s1)
      #print(dist.probs)
      a = dist.sample()
      if self.action_is_discrete:
        a = int(a)
      else:
        a = torch.min(torch.max(a,self.action_low),self.action_high) #clip action

    sarst = (s1,a) #t = termination

    s2,r,done,info = self.env.step(a)
    if render:
      self.env.render()
    sarst += (float(r),s2,1-int(done)) #t = termination signal (t=0 if s2 is terminal state, t=1 otherwise)

    return sarst

  def _process_td_steps(self,step_list,discount_rate):
    # takes a list of SARST steps as input and outputs an n-steps TD SARST tuple
    s0 = step_list[0][0]
    a = step_list[0][1]
    R = np.sum([step_list[i][2]*discount_rate**(i) for i in range(len(step_list))])
    s1 = step_list[-1][3]
    t = step_list[-1][4]

    return (s0,a,R,s1,t)

  def fit(
    self,
    n_episodes,
    max_ts_by_episode,
    discount_rate=0.99,
    tmax = 1,
    reset=False,
    render=False):

    &#34;&#34;&#34;
    Fit the agent 
    
    **Parameters**:

    *n_episodes* (int): number of episodes to run

    *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode

    *discount_rate* (*float*): reward discount rate. Defaults to 0.99

    *tmax* (*int*): number of timesteps between k-step TD updates, k=1,...,tmax. An infinite value implies episodic updates

    *reset* (*boolean*): reset trace, scheduler time counter and learning rate time counter to zero if fit has been called before

    *render* (*boolean*): render environment while fitting

    **Returns**:

    (nn.Module) updated agent

    &#34;&#34;&#34;
    logging.info(&#34;Running AC.fit&#34;)
    if reset:
      self.scheduler.reset()
      self.trace = []
      self.critic.scheduler = optim.scheduler.LambdaLR(self.critic.opt,self.scheduler.critic_lr_scheduler_fn)
      self.actor.scheduler = optim.scheduler.LambdaLR(self.actor.opt,self.scheduler.actor_lr_scheduler_fn)

    scheduler = self.scheduler
    actor = self.actor
    critic = self.critic

    # fit agents
    logging.info(&#34;Training...&#34;)

    for i in range(n_episodes):

      ts_reward = 0

      step_list = deque(maxlen=tmax if isinstance(tmax,int) else None)
      td = 0

      s1 = self.env.reset()

      for j in range(max_ts_by_episode):

        #execute policy
        step_list.append(self._step(actor,critic,s1,render))
        td += 1

        s1 = step_list[-1][3]
        r = step_list[-1][2] #latest reward

        if np.isnan(r):
          raise Exception(&#34;The model diverged; decreasing step sizes or increasing the minimum allowed variance of continuous policies can help to prevent this.&#34;)
        
        done = (step_list[-1][4] == 0)

        if td == tmax:
          processed = [self._process_td_steps([step_list[j] for j in range(i,tmax)],discount_rate) for i in range(tmax)]
          self._update(actor,critic,processed,discount_rate,optimise=True)
          td = 0

        # trace information
        ts_reward += r

        # update learning rate and other hyperparameters
        actor._step()
        critic._step()
        scheduler._step()

        if done:

          break

      if td &gt; 0:
        processed = [self._process_td_steps([step_list[-td+j] for j in range(i,td)],discount_rate) for i in range(td)]
        self._update(actor,critic,processed,discount_rate,optimise=True)

      self.mean_trace.append(ts_reward)
      logging.info(&#34;episode {n}, timestep {ts}, mean reward {x}&#34;.format(n=i,x=ts_reward,ts=scheduler.counter))

    self.actor = actor
    self.critic = critic

    if render:
      self.env.close()

  def plot(self):
    &#34;&#34;&#34;plot mean timestep reward from last `fit` call

    &#34;&#34;&#34;

    if len(self.mean_trace)==0:
      print(&#34;The trace is empty.&#34;)
    else:
      df = pd.DataFrame({
        &#34;episode&#34;:list(range(len(self.mean_trace))),
        &#34;mean_reward&#34;: self.mean_trace})

    ax = sns.lineplot(data=df,x=&#34;episode&#34;,y=&#34;mean_reward&#34;)
    ax.set(xlabel=&#39;episode&#39;, ylabel=&#39;Mean episodic reward&#39;)
    plt.show()

  def play(self,n=200):
    &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
    
    **Parameters**:

    *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

    &#34;&#34;&#34;
    with torch.no_grad():
      obs = self.env.reset()
      for k in range(n):
        pi = self.actor.model.forward(obs)
        action = pi.sample()
        if self.action_is_discrete:
          action = int(action)
        obs,reward,done,info = self.env.step(action)
        self.env.render()
        if done:
          break
      self.env.close()

  def forward(self,x):
    &#34;&#34;&#34;evaluate input with agent

    **Parameters**:

    *x* (*torch.Tensor*): input vector

    &#34;&#34;&#34;
    if isinstance(x,np.ndarray):
      x = torch.from_numpy(x).float()
    return self.agent.model.forward(x)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="rlmodels.models.AC.AC.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, n_episodes, max_ts_by_episode, discount_rate=0.99, tmax=1, reset=False, render=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit the agent </p>
<p><strong>Parameters</strong>:</p>
<p><em>n_episodes</em> (int): number of episodes to run</p>
<p><em>max_ts_by_episodes</em> (<em>int</em>): maximum number of timesteps to run per episode</p>
<p><em>discount_rate</em> (<em>float</em>): reward discount rate. Defaults to 0.99</p>
<p><em>tmax</em> (<em>int</em>): number of timesteps between k-step TD updates, k=1,&hellip;,tmax. An infinite value implies episodic updates</p>
<p><em>reset</em> (<em>boolean</em>): reset trace, scheduler time counter and learning rate time counter to zero if fit has been called before</p>
<p><em>render</em> (<em>boolean</em>): render environment while fitting</p>
<p><strong>Returns</strong>:</p>
<p>(nn.Module) updated agent</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit(
  self,
  n_episodes,
  max_ts_by_episode,
  discount_rate=0.99,
  tmax = 1,
  reset=False,
  render=False):

  &#34;&#34;&#34;
  Fit the agent 
  
  **Parameters**:

  *n_episodes* (int): number of episodes to run

  *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode

  *discount_rate* (*float*): reward discount rate. Defaults to 0.99

  *tmax* (*int*): number of timesteps between k-step TD updates, k=1,...,tmax. An infinite value implies episodic updates

  *reset* (*boolean*): reset trace, scheduler time counter and learning rate time counter to zero if fit has been called before

  *render* (*boolean*): render environment while fitting

  **Returns**:

  (nn.Module) updated agent

  &#34;&#34;&#34;
  logging.info(&#34;Running AC.fit&#34;)
  if reset:
    self.scheduler.reset()
    self.trace = []
    self.critic.scheduler = optim.scheduler.LambdaLR(self.critic.opt,self.scheduler.critic_lr_scheduler_fn)
    self.actor.scheduler = optim.scheduler.LambdaLR(self.actor.opt,self.scheduler.actor_lr_scheduler_fn)

  scheduler = self.scheduler
  actor = self.actor
  critic = self.critic

  # fit agents
  logging.info(&#34;Training...&#34;)

  for i in range(n_episodes):

    ts_reward = 0

    step_list = deque(maxlen=tmax if isinstance(tmax,int) else None)
    td = 0

    s1 = self.env.reset()

    for j in range(max_ts_by_episode):

      #execute policy
      step_list.append(self._step(actor,critic,s1,render))
      td += 1

      s1 = step_list[-1][3]
      r = step_list[-1][2] #latest reward

      if np.isnan(r):
        raise Exception(&#34;The model diverged; decreasing step sizes or increasing the minimum allowed variance of continuous policies can help to prevent this.&#34;)
      
      done = (step_list[-1][4] == 0)

      if td == tmax:
        processed = [self._process_td_steps([step_list[j] for j in range(i,tmax)],discount_rate) for i in range(tmax)]
        self._update(actor,critic,processed,discount_rate,optimise=True)
        td = 0

      # trace information
      ts_reward += r

      # update learning rate and other hyperparameters
      actor._step()
      critic._step()
      scheduler._step()

      if done:

        break

    if td &gt; 0:
      processed = [self._process_td_steps([step_list[-td+j] for j in range(i,td)],discount_rate) for i in range(td)]
      self._update(actor,critic,processed,discount_rate,optimise=True)

    self.mean_trace.append(ts_reward)
    logging.info(&#34;episode {n}, timestep {ts}, mean reward {x}&#34;.format(n=i,x=ts_reward,ts=scheduler.counter))

  self.actor = actor
  self.critic = critic

  if render:
    self.env.close()</code></pre>
</details>
</dd>
<dt id="rlmodels.models.AC.AC.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"><p>evaluate input with agent</p>
<p><strong>Parameters</strong>:</p>
<p><em>x</em> (<em>torch.Tensor</em>): input vector</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self,x):
  &#34;&#34;&#34;evaluate input with agent

  **Parameters**:

  *x* (*torch.Tensor*): input vector

  &#34;&#34;&#34;
  if isinstance(x,np.ndarray):
    x = torch.from_numpy(x).float()
  return self.agent.model.forward(x)</code></pre>
</details>
</dd>
<dt id="rlmodels.models.AC.AC.play"><code class="name flex">
<span>def <span class="ident">play</span></span>(<span>self, n=200)</span>
</code></dt>
<dd>
<section class="desc"><p>show agent's animation. Only works for OpenAI environments</p>
<p><strong>Parameters</strong>:</p>
<p><em>n</em> (<em>int</em>): maximum number of timesteps to visualise. Defaults to 200</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def play(self,n=200):
  &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
  
  **Parameters**:

  *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

  &#34;&#34;&#34;
  with torch.no_grad():
    obs = self.env.reset()
    for k in range(n):
      pi = self.actor.model.forward(obs)
      action = pi.sample()
      if self.action_is_discrete:
        action = int(action)
      obs,reward,done,info = self.env.step(action)
      self.env.render()
      if done:
        break
    self.env.close()</code></pre>
</details>
</dd>
<dt id="rlmodels.models.AC.AC.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>plot mean timestep reward from last <code>fit</code> call</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plot(self):
  &#34;&#34;&#34;plot mean timestep reward from last `fit` call

  &#34;&#34;&#34;

  if len(self.mean_trace)==0:
    print(&#34;The trace is empty.&#34;)
  else:
    df = pd.DataFrame({
      &#34;episode&#34;:list(range(len(self.mean_trace))),
      &#34;mean_reward&#34;: self.mean_trace})

  ax = sns.lineplot(data=df,x=&#34;episode&#34;,y=&#34;mean_reward&#34;)
  ax.set(xlabel=&#39;episode&#39;, ylabel=&#39;Mean episodic reward&#39;)
  plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="rlmodels.models.AC.ACScheduler"><code class="flex name class">
<span>class <span class="ident">ACScheduler</span></span>
<span>(</span><span>actor_lr_scheduler_fn=None, critic_lr_scheduler_fn=None)</span>
</code></dt>
<dd>
<section class="desc"><p>AC hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global timestep counter.
At each step it sets the hyperparameter values given by the provided functons</p>
<p><strong>Parameters</strong>:</p>
<p><em>actor_lr_scheduler_func</em> (<em>function</em>): multiplicative lr update for actor</p>
<p><em>critic_lr_scheduler_func</em> (<em>function</em>): multiplicative lr update for critic</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class ACScheduler(object):
  &#34;&#34;&#34;AC hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global timestep counter.
  At each step it sets the hyperparameter values given by the provided functons

  **Parameters**:
  
  *actor_lr_scheduler_func* (*function*): multiplicative lr update for actor

  *critic_lr_scheduler_func* (*function*): multiplicative lr update for critic

  &#34;&#34;&#34;
  def __init__(
    self,
    actor_lr_scheduler_fn = None,
    critic_lr_scheduler_fn = None):

    self.critic_lr_scheduler_fn = critic_lr_scheduler_fn
    self.actor_lr_scheduler_fn = actor_lr_scheduler_fn

    self.reset()

  def _step(self):

    self.counter += 1

  def reset(self):

    &#34;&#34;&#34;reset iteration counter
  
    &#34;&#34;&#34;
    self.counter = 0

    self._step()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="rlmodels.models.AC.ACScheduler.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>reset iteration counter</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def reset(self):

  &#34;&#34;&#34;reset iteration counter

  &#34;&#34;&#34;
  self.counter = 0

  self._step()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rlmodels.models" href="index.html">rlmodels.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rlmodels.models.AC.AC" href="#rlmodels.models.AC.AC">AC</a></code></h4>
<ul class="">
<li><code><a title="rlmodels.models.AC.AC.fit" href="#rlmodels.models.AC.AC.fit">fit</a></code></li>
<li><code><a title="rlmodels.models.AC.AC.forward" href="#rlmodels.models.AC.AC.forward">forward</a></code></li>
<li><code><a title="rlmodels.models.AC.AC.play" href="#rlmodels.models.AC.AC.play">play</a></code></li>
<li><code><a title="rlmodels.models.AC.AC.plot" href="#rlmodels.models.AC.AC.plot">plot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="rlmodels.models.AC.ACScheduler" href="#rlmodels.models.AC.ACScheduler">ACScheduler</a></code></h4>
<ul class="">
<li><code><a title="rlmodels.models.AC.ACScheduler.reset" href="#rlmodels.models.AC.ACScheduler.reset">reset</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>