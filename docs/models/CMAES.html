<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>rlmodels.models.CMAES API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rlmodels.models.CMAES</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import random
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import logging

class CMAESScheduler(object):
  &#34;&#34;&#34;CMAES hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global generation counter.
  At each generation it sets the hyperparameter values given by the provided functons

  **Parameters**:
  
  *alpha_mu* (*function*): step size scheduler for the mean parameter 

  *alpha_cm* (*function*): step size scheduler for the covariance matrix parameter

  *beta_mu* (*function*): momentum term for the mean vector parameter

  *beta_cm* (*function*): momentum term for covariance matrix parameter

  &#34;&#34;&#34;
  def __init__(
    self,
    alpha_mu,
    alpha_cm,
    beta_mu,
    beta_cm):

    self.alpha_mu_f = alpha_mu
    self.alpha_cm_f = alpha_cm
    self.beta_mu_f = beta_mu
    self.beta_cm_f = beta_cm

    self.reset()

  def _step(self):

    self.alpha_mu = self.alpha_mu_f(self.counter)
    self.alpha_cm = self.alpha_cm_f(self.counter)
    self.beta_mu = self.beta_mu_f(self.counter)
    self.beta_cm = self.beta_cm_f(self.counter)

    self.counter += 1

  def reset(self):

    &#34;&#34;&#34;reset iteration counter
  
    &#34;&#34;&#34;
    self.counter = 0

    self._step()


class CMAES(object):
  &#34;&#34;&#34;correlation matrix adaptive evolutionary strategy algorithm 
  
  **Parameters**: 

  *agent* (*torch.nn.Module*): Pytorch neural network model 

  *env*: environment class with roughly the same interface as OpenAI gym&#39;s environments, particularly the step() method

  *scheduler* (`CMAESScheduler`): scheduler object that controls hyperparameter values at runtime

  &#34;&#34;&#34;
  
  def __init__(self,agent,env,scheduler):

    self.agent = agent
    # reference architecture architecture
    self.architecture = self.agent.state_dict()

    # get parameter space dimensionality
    d = 0
    for layer in self.architecture:
      d += np.prod(self.architecture[layer].shape)
    self.d = d

    self.env = env
    self.scheduler = scheduler
    self.max_trace = []
    self.mean_trace = []

    #initialise mean and covariance matrix
    self.mu = torch.from_numpy(np.zeros((self.d,1))).float()
    self.cm = torch.from_numpy(np.eye(self.d)).float()

    #initialise mean and covariance momentum terms
    self.update_mu = torch.from_numpy(np.zeros((self.d,1))).float()
    self.update_cm = torch.from_numpy(np.zeros(self.d,self.d)).float()

  def _unroll_params(self,population):
    # unroll neural architecture weights into a long vector
    # OUTPUT
    # matrix whose columns are the population parameter vectors
    unrolled_matrix = torch.empty(self.d,0)
    for ind in population:
      architecture = ind[&#34;architecture&#34;]
      unrolled = torch.empty(0,1)
      for layer in architecture:
        unrolled = torch.cat([unrolled,architecture[layer].view(-1,1)],0)
      unrolled_matrix = torch.cat([unrolled_matrix,unrolled],1)

    return unrolled_matrix

  def _get_population_statistics(self,population):
    # OUTPUT
    # weighted population mean
    # aggregated rank 1 updates for covariance matrix

    n = len(population)
    unrolled_matrix = self._unroll_params(population)
    weights = torch.from_numpy(np.array([ind[&#34;weight&#34;] for ind in population]).reshape(-1,1)).float()
    
    # compute weighted mean as a matrix vector product
    w_mean = torch.mm(unrolled_matrix,weights)

    m_y, n_y = unrolled_matrix.shape

    y = (unrolled_matrix - self.mu)

    r1updates = torch.zeros(m_y,m_y)

    for i in range(n_y):
      col = y[:,i]
      r1updates += weights[i]*torch.ger(col,col) 

    return w_mean, r1updates

  def _roll(self,unrolled):
    # roll a long vector into the agent&#39;s structure
    architecture = self.architecture
    rolled = {}
    s0=0
    for layer in architecture:
      if len(architecture[layer].shape) == 2:
        m,n = architecture[layer].shape
        rolled[layer] = unrolled[s0:(s0+m*n)].view(m,n)
      else:
        m = architecture[layer].shape[0]
        n = 1
        rolled[layer] = unrolled[s0:(s0+m*n)].view(m)
      
      s0 += m*n
    return rolled 

  def _create_population(self,n):
    population = []
    for i in range(n):
      eps = np.random.multivariate_normal(self.mu.numpy()[:,0],self.cm.numpy(),1)
      torch_eps = torch.from_numpy(eps).float().view(self.d,1)
      ind_architecture = self._roll(torch_eps)
      population.append({
          &#34;architecture&#34;:ind_architecture,
          &#34;avg_episode_r&#34;:0})

    return population

  def _calculate_rank(self,vector):
    # calculate vector ranks from lowest(1) to highest (len(vector))

    a={}
    rank=1
    for num in sorted(vector):
      if num not in a:
        a[num]=rank
        rank=rank+1
    return np.array([a[i] for i in vector])

  def fit(self,
      weight_func=None,
      n_generations=100,
      individuals_by_gen=20,
      episodes_by_ind=10,
      max_ts_by_episode=200,
      reset=False):

    &#34;&#34;&#34;Fit the agent 
  
    **Parameters**:
    
    *weight_func* (*function*): function that maps individual ranked (lowest to highest) performances to (normalised to sum 1) recombination weights. It has to work on *numpy* arrays; defaults to quadratic function

    *n_generations* (*int*): maximum number of generations to run. Defaults to 100

    *individuals_by_gen* (*int*): population size for each generation. Defaults to 20

    *episodes_by_ind* (*int*): how many episodes to run for each individual in the population. Defaults to 10

    *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode. Defaults to 200

    *reset* (*boolean*): reset scheduler counter to zero and performance traces if *fit* has been called before
  
    **Returns**: 

    (*torch nn.Module*) best-performing agent from last generation

    &#34;&#34;&#34;
    if reset:
      self.scheduler.reset()
      self.mean_trace = []
      self.max_trace = []
    #weight_func defaults to normalised squared ranks

    scheduler = self.scheduler

    if weight_func is None:
      # default to quadratic rank as fitness
      def weight_func(ranks):
        return ranks**2


    #reference architecture structure
    architecture = self.architecture

    population = self._create_population(individuals_by_gen)
      
    # evaluate population
    i = 0
    best = -np.Inf

    for i in tqdm(range(n_generations)):

      for l in range(len(population)):
        # set up nn agent
        agent = population[l]

        self.agent.load_state_dict(agent[&#34;architecture&#34;])

        #interact with environment
        for j in range(episodes_by_ind):
          
          ep_reward = 0 
          
          obs = self.env.reset()
          
          for k in range(max_ts_by_episode):
            with torch.no_grad():
              action = self.agent.forward(obs)
            obs,reward,done,info = self.env.step(action)
            
            ep_reward += reward #avg intra episode reward

            if done:
              break

          population[l][&#34;avg_episode_r&#34;] += ep_reward/episodes_by_ind #avg reward

      # calculate weights for each individual
      population_rewards = np.array([ind[&#34;avg_episode_r&#34;] for ind in population])
      weights = weight_func(self._calculate_rank(population_rewards))

      if ((np.argsort(population_rewards) - np.argsort(weights)) != 0).any():
        logging.warning(&#34;Warning: recombination weights function does not preserve rank order&#34;)

      norm_weights = weights/np.sum(weights)

      #print(population_rewards)
      #print(norm_weights)

      for k in range(len(population)):
        population[k][&#34;weight&#34;] = norm_weights[k]

      #debug info
      self.mean_trace.append(np.mean(population_rewards))
      self.max_trace.append(np.max(population_rewards))
      logging.info(&#34;generation {n}, mean trace {x}, max trace {y}&#34;.format(n=i,x=np.mean(population_rewards),y=np.max(population_rewards)))

      w_mean, r1updates = self._get_population_statistics(population)

      #update gradient with momentum
      self.update_cm = scheduler.beta_cm*self.update_cm + r1updates - self.cm
      self.update_mu = scheduler.beta_mu*self.update_mu + w_mean - self.mu

      #update parameters
      self.cm = self.cm + scheduler.alpha_cm*self.update_cm
      self.mu = self.mu + scheduler.alpha_mu*self.update_mu

      # update agent to the best performing one in current population
      self.agent.load_state_dict(population[np.argmax(norm_weights)][&#34;architecture&#34;])

      population = self._create_population(individuals_by_gen)
      best = np.max(population_rewards) # best avg episodic reward 

      scheduler._step()

    return self.agent

  def plot(self):
    &#34;&#34;&#34;plot mean and max episodic reward for each generation from last `fit` call

    &#34;&#34;&#34;
    if len(self.mean_trace)==0:
      print(&#34;The traces are empty.&#34;)
    else:
      df = pd.DataFrame({
        &#34;generation&#34;:list(range(len(self.max_trace))) + list(range(len(self.max_trace))),
        &#34;value&#34;: self.max_trace + self.mean_trace,
        &#34;trace&#34;: [&#34;max&#34; for x in self.max_trace] + [&#34;mean&#34; for x in self.mean_trace]})

      ax = sns.lineplot(data=df,x=&#34;generation&#34;,y=&#34;value&#34;,hue=&#34;trace&#34;)
      ax.set(xlabel=&#39;generation&#39;, ylabel=&#39;Mean episodic reward&#39;)
      plt.show()

  def play(self,n=200):
    &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
    
    **Parameters**:

    *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

    &#34;&#34;&#34;

    obs = self.env.reset()
    with torch.no_grad():
      for k in range(n):
        action = self.agent.forward(obs)
        obs,reward,done,info = self.env.step(action)
        self.env.render()
        if done:
          break
      self.env.close()

  def forward(self,x):
    &#34;&#34;&#34;evaluate input with agent

    **Parameters**:

    *x* (*torch.Tensor*): input vector

    &#34;&#34;&#34;
    if isinstance(x,np.ndarray):
      x = torch.from_numpy(x).float()
    return self.agent.forward(x)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rlmodels.models.CMAES.CMAES"><code class="flex name class">
<span>class <span class="ident">CMAES</span></span>
<span>(</span><span>agent, env, scheduler)</span>
</code></dt>
<dd>
<div class="desc"><p>correlation matrix adaptive evolutionary strategy algorithm </p>
<p><strong>Parameters</strong>: </p>
<p><em>agent</em> (<em>torch.nn.Module</em>): Pytorch neural network model </p>
<p><em>env</em>: environment class with roughly the same interface as OpenAI gym's environments, particularly the step() method</p>
<p><em>scheduler</em> (<code><a title="rlmodels.models.CMAES.CMAESScheduler" href="#rlmodels.models.CMAES.CMAESScheduler">CMAESScheduler</a></code>): scheduler object that controls hyperparameter values at runtime</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CMAES(object):
  &#34;&#34;&#34;correlation matrix adaptive evolutionary strategy algorithm 
  
  **Parameters**: 

  *agent* (*torch.nn.Module*): Pytorch neural network model 

  *env*: environment class with roughly the same interface as OpenAI gym&#39;s environments, particularly the step() method

  *scheduler* (`CMAESScheduler`): scheduler object that controls hyperparameter values at runtime

  &#34;&#34;&#34;
  
  def __init__(self,agent,env,scheduler):

    self.agent = agent
    # reference architecture architecture
    self.architecture = self.agent.state_dict()

    # get parameter space dimensionality
    d = 0
    for layer in self.architecture:
      d += np.prod(self.architecture[layer].shape)
    self.d = d

    self.env = env
    self.scheduler = scheduler
    self.max_trace = []
    self.mean_trace = []

    #initialise mean and covariance matrix
    self.mu = torch.from_numpy(np.zeros((self.d,1))).float()
    self.cm = torch.from_numpy(np.eye(self.d)).float()

    #initialise mean and covariance momentum terms
    self.update_mu = torch.from_numpy(np.zeros((self.d,1))).float()
    self.update_cm = torch.from_numpy(np.zeros(self.d,self.d)).float()

  def _unroll_params(self,population):
    # unroll neural architecture weights into a long vector
    # OUTPUT
    # matrix whose columns are the population parameter vectors
    unrolled_matrix = torch.empty(self.d,0)
    for ind in population:
      architecture = ind[&#34;architecture&#34;]
      unrolled = torch.empty(0,1)
      for layer in architecture:
        unrolled = torch.cat([unrolled,architecture[layer].view(-1,1)],0)
      unrolled_matrix = torch.cat([unrolled_matrix,unrolled],1)

    return unrolled_matrix

  def _get_population_statistics(self,population):
    # OUTPUT
    # weighted population mean
    # aggregated rank 1 updates for covariance matrix

    n = len(population)
    unrolled_matrix = self._unroll_params(population)
    weights = torch.from_numpy(np.array([ind[&#34;weight&#34;] for ind in population]).reshape(-1,1)).float()
    
    # compute weighted mean as a matrix vector product
    w_mean = torch.mm(unrolled_matrix,weights)

    m_y, n_y = unrolled_matrix.shape

    y = (unrolled_matrix - self.mu)

    r1updates = torch.zeros(m_y,m_y)

    for i in range(n_y):
      col = y[:,i]
      r1updates += weights[i]*torch.ger(col,col) 

    return w_mean, r1updates

  def _roll(self,unrolled):
    # roll a long vector into the agent&#39;s structure
    architecture = self.architecture
    rolled = {}
    s0=0
    for layer in architecture:
      if len(architecture[layer].shape) == 2:
        m,n = architecture[layer].shape
        rolled[layer] = unrolled[s0:(s0+m*n)].view(m,n)
      else:
        m = architecture[layer].shape[0]
        n = 1
        rolled[layer] = unrolled[s0:(s0+m*n)].view(m)
      
      s0 += m*n
    return rolled 

  def _create_population(self,n):
    population = []
    for i in range(n):
      eps = np.random.multivariate_normal(self.mu.numpy()[:,0],self.cm.numpy(),1)
      torch_eps = torch.from_numpy(eps).float().view(self.d,1)
      ind_architecture = self._roll(torch_eps)
      population.append({
          &#34;architecture&#34;:ind_architecture,
          &#34;avg_episode_r&#34;:0})

    return population

  def _calculate_rank(self,vector):
    # calculate vector ranks from lowest(1) to highest (len(vector))

    a={}
    rank=1
    for num in sorted(vector):
      if num not in a:
        a[num]=rank
        rank=rank+1
    return np.array([a[i] for i in vector])

  def fit(self,
      weight_func=None,
      n_generations=100,
      individuals_by_gen=20,
      episodes_by_ind=10,
      max_ts_by_episode=200,
      reset=False):

    &#34;&#34;&#34;Fit the agent 
  
    **Parameters**:
    
    *weight_func* (*function*): function that maps individual ranked (lowest to highest) performances to (normalised to sum 1) recombination weights. It has to work on *numpy* arrays; defaults to quadratic function

    *n_generations* (*int*): maximum number of generations to run. Defaults to 100

    *individuals_by_gen* (*int*): population size for each generation. Defaults to 20

    *episodes_by_ind* (*int*): how many episodes to run for each individual in the population. Defaults to 10

    *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode. Defaults to 200

    *reset* (*boolean*): reset scheduler counter to zero and performance traces if *fit* has been called before
  
    **Returns**: 

    (*torch nn.Module*) best-performing agent from last generation

    &#34;&#34;&#34;
    if reset:
      self.scheduler.reset()
      self.mean_trace = []
      self.max_trace = []
    #weight_func defaults to normalised squared ranks

    scheduler = self.scheduler

    if weight_func is None:
      # default to quadratic rank as fitness
      def weight_func(ranks):
        return ranks**2


    #reference architecture structure
    architecture = self.architecture

    population = self._create_population(individuals_by_gen)
      
    # evaluate population
    i = 0
    best = -np.Inf

    for i in tqdm(range(n_generations)):

      for l in range(len(population)):
        # set up nn agent
        agent = population[l]

        self.agent.load_state_dict(agent[&#34;architecture&#34;])

        #interact with environment
        for j in range(episodes_by_ind):
          
          ep_reward = 0 
          
          obs = self.env.reset()
          
          for k in range(max_ts_by_episode):
            with torch.no_grad():
              action = self.agent.forward(obs)
            obs,reward,done,info = self.env.step(action)
            
            ep_reward += reward #avg intra episode reward

            if done:
              break

          population[l][&#34;avg_episode_r&#34;] += ep_reward/episodes_by_ind #avg reward

      # calculate weights for each individual
      population_rewards = np.array([ind[&#34;avg_episode_r&#34;] for ind in population])
      weights = weight_func(self._calculate_rank(population_rewards))

      if ((np.argsort(population_rewards) - np.argsort(weights)) != 0).any():
        logging.warning(&#34;Warning: recombination weights function does not preserve rank order&#34;)

      norm_weights = weights/np.sum(weights)

      #print(population_rewards)
      #print(norm_weights)

      for k in range(len(population)):
        population[k][&#34;weight&#34;] = norm_weights[k]

      #debug info
      self.mean_trace.append(np.mean(population_rewards))
      self.max_trace.append(np.max(population_rewards))
      logging.info(&#34;generation {n}, mean trace {x}, max trace {y}&#34;.format(n=i,x=np.mean(population_rewards),y=np.max(population_rewards)))

      w_mean, r1updates = self._get_population_statistics(population)

      #update gradient with momentum
      self.update_cm = scheduler.beta_cm*self.update_cm + r1updates - self.cm
      self.update_mu = scheduler.beta_mu*self.update_mu + w_mean - self.mu

      #update parameters
      self.cm = self.cm + scheduler.alpha_cm*self.update_cm
      self.mu = self.mu + scheduler.alpha_mu*self.update_mu

      # update agent to the best performing one in current population
      self.agent.load_state_dict(population[np.argmax(norm_weights)][&#34;architecture&#34;])

      population = self._create_population(individuals_by_gen)
      best = np.max(population_rewards) # best avg episodic reward 

      scheduler._step()

    return self.agent

  def plot(self):
    &#34;&#34;&#34;plot mean and max episodic reward for each generation from last `fit` call

    &#34;&#34;&#34;
    if len(self.mean_trace)==0:
      print(&#34;The traces are empty.&#34;)
    else:
      df = pd.DataFrame({
        &#34;generation&#34;:list(range(len(self.max_trace))) + list(range(len(self.max_trace))),
        &#34;value&#34;: self.max_trace + self.mean_trace,
        &#34;trace&#34;: [&#34;max&#34; for x in self.max_trace] + [&#34;mean&#34; for x in self.mean_trace]})

      ax = sns.lineplot(data=df,x=&#34;generation&#34;,y=&#34;value&#34;,hue=&#34;trace&#34;)
      ax.set(xlabel=&#39;generation&#39;, ylabel=&#39;Mean episodic reward&#39;)
      plt.show()

  def play(self,n=200):
    &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
    
    **Parameters**:

    *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

    &#34;&#34;&#34;

    obs = self.env.reset()
    with torch.no_grad():
      for k in range(n):
        action = self.agent.forward(obs)
        obs,reward,done,info = self.env.step(action)
        self.env.render()
        if done:
          break
      self.env.close()

  def forward(self,x):
    &#34;&#34;&#34;evaluate input with agent

    **Parameters**:

    *x* (*torch.Tensor*): input vector

    &#34;&#34;&#34;
    if isinstance(x,np.ndarray):
      x = torch.from_numpy(x).float()
    return self.agent.forward(x)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="rlmodels.models.CMAES.CMAES.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, weight_func=None, n_generations=100, individuals_by_gen=20, episodes_by_ind=10, max_ts_by_episode=200, reset=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit the agent </p>
<p><strong>Parameters</strong>:</p>
<p><em>weight_func</em> (<em>function</em>): function that maps individual ranked (lowest to highest) performances to (normalised to sum 1) recombination weights. It has to work on <em>numpy</em> arrays; defaults to quadratic function</p>
<p><em>n_generations</em> (<em>int</em>): maximum number of generations to run. Defaults to 100</p>
<p><em>individuals_by_gen</em> (<em>int</em>): population size for each generation. Defaults to 20</p>
<p><em>episodes_by_ind</em> (<em>int</em>): how many episodes to run for each individual in the population. Defaults to 10</p>
<p><em>max_ts_by_episodes</em> (<em>int</em>): maximum number of timesteps to run per episode. Defaults to 200</p>
<p><em>reset</em> (<em>boolean</em>): reset scheduler counter to zero and performance traces if <em>fit</em> has been called before</p>
<p><strong>Returns</strong>: </p>
<p>(<em>torch nn.Module</em>) best-performing agent from last generation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self,
    weight_func=None,
    n_generations=100,
    individuals_by_gen=20,
    episodes_by_ind=10,
    max_ts_by_episode=200,
    reset=False):

  &#34;&#34;&#34;Fit the agent 

  **Parameters**:
  
  *weight_func* (*function*): function that maps individual ranked (lowest to highest) performances to (normalised to sum 1) recombination weights. It has to work on *numpy* arrays; defaults to quadratic function

  *n_generations* (*int*): maximum number of generations to run. Defaults to 100

  *individuals_by_gen* (*int*): population size for each generation. Defaults to 20

  *episodes_by_ind* (*int*): how many episodes to run for each individual in the population. Defaults to 10

  *max_ts_by_episodes* (*int*): maximum number of timesteps to run per episode. Defaults to 200

  *reset* (*boolean*): reset scheduler counter to zero and performance traces if *fit* has been called before

  **Returns**: 

  (*torch nn.Module*) best-performing agent from last generation

  &#34;&#34;&#34;
  if reset:
    self.scheduler.reset()
    self.mean_trace = []
    self.max_trace = []
  #weight_func defaults to normalised squared ranks

  scheduler = self.scheduler

  if weight_func is None:
    # default to quadratic rank as fitness
    def weight_func(ranks):
      return ranks**2


  #reference architecture structure
  architecture = self.architecture

  population = self._create_population(individuals_by_gen)
    
  # evaluate population
  i = 0
  best = -np.Inf

  for i in tqdm(range(n_generations)):

    for l in range(len(population)):
      # set up nn agent
      agent = population[l]

      self.agent.load_state_dict(agent[&#34;architecture&#34;])

      #interact with environment
      for j in range(episodes_by_ind):
        
        ep_reward = 0 
        
        obs = self.env.reset()
        
        for k in range(max_ts_by_episode):
          with torch.no_grad():
            action = self.agent.forward(obs)
          obs,reward,done,info = self.env.step(action)
          
          ep_reward += reward #avg intra episode reward

          if done:
            break

        population[l][&#34;avg_episode_r&#34;] += ep_reward/episodes_by_ind #avg reward

    # calculate weights for each individual
    population_rewards = np.array([ind[&#34;avg_episode_r&#34;] for ind in population])
    weights = weight_func(self._calculate_rank(population_rewards))

    if ((np.argsort(population_rewards) - np.argsort(weights)) != 0).any():
      logging.warning(&#34;Warning: recombination weights function does not preserve rank order&#34;)

    norm_weights = weights/np.sum(weights)

    #print(population_rewards)
    #print(norm_weights)

    for k in range(len(population)):
      population[k][&#34;weight&#34;] = norm_weights[k]

    #debug info
    self.mean_trace.append(np.mean(population_rewards))
    self.max_trace.append(np.max(population_rewards))
    logging.info(&#34;generation {n}, mean trace {x}, max trace {y}&#34;.format(n=i,x=np.mean(population_rewards),y=np.max(population_rewards)))

    w_mean, r1updates = self._get_population_statistics(population)

    #update gradient with momentum
    self.update_cm = scheduler.beta_cm*self.update_cm + r1updates - self.cm
    self.update_mu = scheduler.beta_mu*self.update_mu + w_mean - self.mu

    #update parameters
    self.cm = self.cm + scheduler.alpha_cm*self.update_cm
    self.mu = self.mu + scheduler.alpha_mu*self.update_mu

    # update agent to the best performing one in current population
    self.agent.load_state_dict(population[np.argmax(norm_weights)][&#34;architecture&#34;])

    population = self._create_population(individuals_by_gen)
    best = np.max(population_rewards) # best avg episodic reward 

    scheduler._step()

  return self.agent</code></pre>
</details>
</dd>
<dt id="rlmodels.models.CMAES.CMAES.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>evaluate input with agent</p>
<p><strong>Parameters</strong>:</p>
<p><em>x</em> (<em>torch.Tensor</em>): input vector</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,x):
  &#34;&#34;&#34;evaluate input with agent

  **Parameters**:

  *x* (*torch.Tensor*): input vector

  &#34;&#34;&#34;
  if isinstance(x,np.ndarray):
    x = torch.from_numpy(x).float()
  return self.agent.forward(x)</code></pre>
</details>
</dd>
<dt id="rlmodels.models.CMAES.CMAES.play"><code class="name flex">
<span>def <span class="ident">play</span></span>(<span>self, n=200)</span>
</code></dt>
<dd>
<div class="desc"><p>show agent's animation. Only works for OpenAI environments</p>
<p><strong>Parameters</strong>:</p>
<p><em>n</em> (<em>int</em>): maximum number of timesteps to visualise. Defaults to 200</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def play(self,n=200):
  &#34;&#34;&#34;show agent&#39;s animation. Only works for OpenAI environments
  
  **Parameters**:

  *n* (*int*): maximum number of timesteps to visualise. Defaults to 200

  &#34;&#34;&#34;

  obs = self.env.reset()
  with torch.no_grad():
    for k in range(n):
      action = self.agent.forward(obs)
      obs,reward,done,info = self.env.step(action)
      self.env.render()
      if done:
        break
    self.env.close()</code></pre>
</details>
</dd>
<dt id="rlmodels.models.CMAES.CMAES.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>plot mean and max episodic reward for each generation from last <code>fit</code> call</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self):
  &#34;&#34;&#34;plot mean and max episodic reward for each generation from last `fit` call

  &#34;&#34;&#34;
  if len(self.mean_trace)==0:
    print(&#34;The traces are empty.&#34;)
  else:
    df = pd.DataFrame({
      &#34;generation&#34;:list(range(len(self.max_trace))) + list(range(len(self.max_trace))),
      &#34;value&#34;: self.max_trace + self.mean_trace,
      &#34;trace&#34;: [&#34;max&#34; for x in self.max_trace] + [&#34;mean&#34; for x in self.mean_trace]})

    ax = sns.lineplot(data=df,x=&#34;generation&#34;,y=&#34;value&#34;,hue=&#34;trace&#34;)
    ax.set(xlabel=&#39;generation&#39;, ylabel=&#39;Mean episodic reward&#39;)
    plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="rlmodels.models.CMAES.CMAESScheduler"><code class="flex name class">
<span>class <span class="ident">CMAESScheduler</span></span>
<span>(</span><span>alpha_mu, alpha_cm, beta_mu, beta_cm)</span>
</code></dt>
<dd>
<div class="desc"><p>CMAES hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global generation counter.
At each generation it sets the hyperparameter values given by the provided functons</p>
<p><strong>Parameters</strong>:</p>
<p><em>alpha_mu</em> (<em>function</em>): step size scheduler for the mean parameter </p>
<p><em>alpha_cm</em> (<em>function</em>): step size scheduler for the covariance matrix parameter</p>
<p><em>beta_mu</em> (<em>function</em>): momentum term for the mean vector parameter</p>
<p><em>beta_cm</em> (<em>function</em>): momentum term for covariance matrix parameter</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CMAESScheduler(object):
  &#34;&#34;&#34;CMAES hyperparameter scheduler. It allows to modify hyperparameters at runtime as a function of a global generation counter.
  At each generation it sets the hyperparameter values given by the provided functons

  **Parameters**:
  
  *alpha_mu* (*function*): step size scheduler for the mean parameter 

  *alpha_cm* (*function*): step size scheduler for the covariance matrix parameter

  *beta_mu* (*function*): momentum term for the mean vector parameter

  *beta_cm* (*function*): momentum term for covariance matrix parameter

  &#34;&#34;&#34;
  def __init__(
    self,
    alpha_mu,
    alpha_cm,
    beta_mu,
    beta_cm):

    self.alpha_mu_f = alpha_mu
    self.alpha_cm_f = alpha_cm
    self.beta_mu_f = beta_mu
    self.beta_cm_f = beta_cm

    self.reset()

  def _step(self):

    self.alpha_mu = self.alpha_mu_f(self.counter)
    self.alpha_cm = self.alpha_cm_f(self.counter)
    self.beta_mu = self.beta_mu_f(self.counter)
    self.beta_cm = self.beta_cm_f(self.counter)

    self.counter += 1

  def reset(self):

    &#34;&#34;&#34;reset iteration counter
  
    &#34;&#34;&#34;
    self.counter = 0

    self._step()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="rlmodels.models.CMAES.CMAESScheduler.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>reset iteration counter</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):

  &#34;&#34;&#34;reset iteration counter

  &#34;&#34;&#34;
  self.counter = 0

  self._step()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rlmodels.models" href="index.html">rlmodels.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rlmodels.models.CMAES.CMAES" href="#rlmodels.models.CMAES.CMAES">CMAES</a></code></h4>
<ul class="">
<li><code><a title="rlmodels.models.CMAES.CMAES.fit" href="#rlmodels.models.CMAES.CMAES.fit">fit</a></code></li>
<li><code><a title="rlmodels.models.CMAES.CMAES.forward" href="#rlmodels.models.CMAES.CMAES.forward">forward</a></code></li>
<li><code><a title="rlmodels.models.CMAES.CMAES.play" href="#rlmodels.models.CMAES.CMAES.play">play</a></code></li>
<li><code><a title="rlmodels.models.CMAES.CMAES.plot" href="#rlmodels.models.CMAES.CMAES.plot">plot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="rlmodels.models.CMAES.CMAESScheduler" href="#rlmodels.models.CMAES.CMAESScheduler">CMAESScheduler</a></code></h4>
<ul class="">
<li><code><a title="rlmodels.models.CMAES.CMAESScheduler.reset" href="#rlmodels.models.CMAES.CMAESScheduler.reset">reset</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>